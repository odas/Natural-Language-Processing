{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not\n",
    "\n",
    "* **OBJECTIVE** (Quoted from Kaggle): \n",
    "    - Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "    - But, it’s not always clear whether a person’s words are actually announcing a disaster.\n",
    "    - In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. \n",
    "    \n",
    "\n",
    "* **DATA** Columns: \n",
    "    - **id** - a unique identifier for each tweet\n",
    "    - **text** - the text of the tweet\n",
    "    - **location** - the location the tweet was sent from (may be blank)\n",
    "    - **keyword** - a particular keyword from the tweet (may be blank)\n",
    "    - **target** - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "    \n",
    "    \n",
    "    \n",
    "* My Approach: \n",
    "    - I used a Target Encoder to convert the 'keywords' into a probability distribution. \n",
    "    - I dropped 'location', as it was a messy column. But I may consider including it down the line. \n",
    "    - While counts of hashtags, mentions, text length etc showed low correlation with target in the training dataset, some additional features have been added to the dataset. \n",
    "    - The 'text' feature has been passed through a TweetTokenizer and lemmatized using the nltk package. \n",
    "    - Hashtags have been extracted into an additional feature column. \n",
    "    - Finally TFIDF has been used on 'hashtags' and cleaned 'text' separately. Along with 'keyword_target' and some additional numerical features - the dataset has been run through hyperopt to identify the best parameter & model - that gives highes f1 score. \n",
    "\n",
    "\n",
    "* **Result**: \n",
    "    - An 85% f1 score was obtained on the training data. \n",
    "    - A 78% fi score was ontained on the test data after submission to Kaggle. \n",
    "    - Further analysis is underway. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Orpita Das\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(500)\n",
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/realornot/train.csv\")\n",
    "test = pd.read_csv(\"../input/realornot/test.csv\")\n",
    "sub = pd.read_csv(\"../input/realornot/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a sense of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train.duplicated()), sum(test.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focussing on individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find blank rows \n",
    "print(train.isnull().any()) \n",
    "print(test.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train.keyword.unique()!=test.keyword.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_d = train[train.target==1].keyword.value_counts().head(10)\n",
    "kw_nd = train[train.target==0].keyword.value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.subplot(121)\n",
    "sns.barplot(kw_d, kw_d.index, color='c')\n",
    "plt.title('Top keywords for disaster tweets')\n",
    "plt.subplot(122)\n",
    "sns.barplot(kw_nd, kw_nd.index, color='y')\n",
    "plt.title('Top keywords for non-disaster tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding\n",
    "encoder = ce.TargetEncoder(cols=['keyword'])\n",
    "encoder.fit(train['keyword'],train['target'])\n",
    "\n",
    "train = train.join(encoder.transform(train['keyword']).add_suffix('_target'))\n",
    "test = test.join(encoder.transform(test['keyword']).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.location.unique()), train.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train.keyword.unique()!=test.keyword.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text : elements & correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text) # Remove link\n",
    "    text = re.sub(r'\\n',' ', text) # Remove line breaks\n",
    "    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n",
    "    return text\n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_mentions(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_links(tweet):\n",
    "    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'\n",
    "\n",
    "def find_emojis(tweet):\n",
    "    emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\" \n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(emoji, tweet)]) or 'no'\n",
    "\n",
    "def process_text(df):\n",
    "    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n",
    "    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n",
    "    df['links'] = df['text'].apply(lambda x: find_links(x))\n",
    "    df['emojis'] = df['text'].apply(lambda x: find_emojis(x))\n",
    "    # df['hashtags'].fillna(value='no', inplace=True)\n",
    "    # df['mentions'].fillna(value='no', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_text(train)\n",
    "test = process_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stat(df):\n",
    "    # Tweet length\n",
    "    df['text_len'] = df['text_clean'].apply(len)\n",
    "    # Word count\n",
    "    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n",
    "    # Stopword count\n",
    "    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n",
    "    # Punctuation count\n",
    "    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    # Count of hashtags (#)\n",
    "    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n",
    "    # Count of mentions (@)\n",
    "    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n",
    "    # Count of links\n",
    "    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n",
    "    # Count of uppercase letters\n",
    "    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n",
    "    # Ratio of uppercase letters\n",
    "    df['caps_ratio'] = df['caps_count'] / df['text_len']\n",
    "    return df\n",
    "\n",
    "train = create_stat(train)\n",
    "test = create_stat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.corr()['target'].drop('target').sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Little correlation, so these above characteristics of text can be dropped.**\n",
    "* **The information in the *mentions* will likely be irrelevant. But the information in the *hashtags* may be useful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any lowercase\n",
    "#print(sum(train.keyword.str.islower()), sum(train.text.str.islower()))\n",
    "\n",
    "# Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "train['text_lower']=[doc.lower() for doc in train.text]\n",
    "test['text_lower']=[doc.lower() for doc in test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "tknzr=TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "train['text_token']=[tknzr.tokenize(doc) for doc in train.text_lower]\n",
    "test['text_token']=[tknzr.tokenize(doc) for doc in test.text_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Orpita Das\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Orpita Das\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Orpita Das\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Orpita Das\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Orpita Das\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Orpita Das\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Orpita Das\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. \n",
    "# By default it is set to Noun. \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:          \n",
    "        return wn.NOUN\n",
    "    \n",
    "train['text_final']=np.zeros(len(train))\n",
    "for i in range(len(train.text_token)):\n",
    "    train.text_final[i]=[t for t in train.text_token[i] if t.isalpha() and t not in stopwords.words('english')]\n",
    "    train.text_final[i]=[lemmatizer.lemmatize(word, nltk_tag_to_wordnet_tag(tag)) for word,tag in pos_tag(train.text_final[i])]\n",
    "    train.text_final[i]=str(train.text_final[i])\n",
    "    \n",
    "test['text_final']=np.zeros(len(test))\n",
    "for i in range(len(test.text_token)):\n",
    "    test.text_final[i]=[t for t in test.text_token[i] if t.isalpha() and t not in stopwords.words('english')]\n",
    "    test.text_final[i]=[lemmatizer.lemmatize(word, nltk_tag_to_wordnet_tag(tag)) for word,tag in pos_tag(test.text_final[i])]\n",
    "    test.text_final[i]=str(test.text_final[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize columns\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000, min_df = 10, ngram_range = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 1070, 'may': 817, 'forest': 511, 'fire': 487, 'near': 883, 'la': 714, 'forest fire': 512, 'ask': 88, 'place': 995, 'officer': 925, 'evacuation': 426, 'order': 944, 'expect': 436, 'people': 977, 'california': 201, 'evacuation order': 427, 'get': 549, 'photo': 983, 'smoke': 1214, 'school': 1155, 'update': 1411, 'hwy': 648, 'close': 248, 'direction': 366, 'due': 391, 'lake': 720, 'county': 288, 'heavy': 605, 'rain': 1056, 'cause': 220, 'flash': 498, 'flood': 503, 'street': 1266, 'colorado': 258, 'spring': 1243, 'area': 75, 'flash flood': 499, 'top': 1353, 'hill': 619, 'see': 1166, 'emergency': 409, 'happen': 588, 'building': 185, 'across': 11, 'tornado': 1355, 'come': 261, 'three': 1337, 'die': 364, 'heat': 603, 'wave': 1457, 'far': 461, 'heat wave': 604, 'haha': 581, 'south': 1230, 'wait': 1438, 'second': 1162, 'live': 766, 'gonna': 562, 'day': 327, 'lose': 779, 'myanmar': 876, 'arrive': 82, 'damage': 318, 'bus': 191, 'car': 210, 'crash': 295, 'school bus': 1156, 'man': 798, 'love': 785, 'summer': 1286, 'fast': 462, 'london': 771, 'cool': 281, 'way': 1460, 'eat': 398, 'shit': 1187, 'last': 727, 'week': 1466, 'like': 757, 'end': 413, 'market': 809, 'ablaze': 5, 'always': 44, 'try': 1386, 'bring': 179, 'break': 175, 'news': 895, 'flag': 495, 'set': 1178, 'aba': 0, 'break news': 176, 'cry': 311, 'side': 1196, 'look': 775, 'sky': 1210, 'night': 902, 'last night': 728, 'build': 184, 'much': 868, 'around': 80, 'new': 890, 'season': 1161, 'office': 924, 'ûò': 1531, 'two': 1398, 'cruz': 310, 'ûó': 1532, 'head': 597, 'st': 1244, 'police': 1011, 'lord': 778, 'check': 232, 'outside': 950, 'alive': 36, 'dead': 328, 'inside': 668, 'awesome': 102, 'time': 1345, 'visit': 1434, 'site': 1207, 'thanks': 1327, 'take': 1306, 'care': 211, 'want': 1444, 'chicago': 234, 'gain': 537, 'know': 713, 'grow': 575, 'west': 1471, 'burn': 187, 'thousand': 1334, 'wildfire': 1480, 'alone': 40, 'perfect': 978, 'life': 753, 'leave': 742, 'first': 491, 'quite': 1052, 'weird': 1468, 'good': 563, 'use': 1416, 'wear': 1462, 'every': 431, 'single': 1201, 'next': 897, 'year': 1512, 'least': 740, 'shoot': 1188, 'home': 628, 'wife': 1476, 'arsonist': 84, 'black': 142, 'north': 903, 'happy': 589, 'train': 1364, 'hard': 590, 'later': 732, 'others': 945, 'front': 523, 'truck': 1382, 'ave': 99, 'heart': 602, 'city': 240, 'upon': 1413, 'tonight': 1352, 'shot': 1190, 'video': 1424, 'mean': 821, 'month': 856, 'student': 1276, 'would': 1499, 'secret': 1163, 'fall': 451, 'something': 1219, 'else': 407, 'huge': 642, 'talk': 1308, 'go': 558, 'make': 794, 'work': 1494, 'kid': 703, 'accident': 8, 'michael': 833, 'traffic': 1361, 'move': 862, 'center': 224, 'lane': 725, 'block': 152, 'great': 570, 'america': 47, 'read': 1065, 'help': 610, 'among': 51, 'teen': 1312, 'report': 1092, 'vehicle': 1419, 'rd': 1062, 'involve': 683, 'please': 1004, 'mile': 837, 'pm': 1007, 'rt': 1129, 'sleep': 1211, 'double': 378, 'risk': 1117, 'scene': 1154, 'owner': 951, 'mom': 853, 'wish': 1487, 'horrible': 631, 'past': 970, 'sunday': 1288, 'finally': 482, 'able': 6, 'thank': 1326, 'god': 560, 'tell': 1313, 'another': 60, 'cross': 307, 'travel': 1375, 'property': 1040, 'dr': 381, 'turn': 1391, 'everyone': 432, 'stop': 1260, 'back': 105, 'delay': 341, 'injury': 667, 'right': 1111, 'consider': 275, 'change': 228, 'support': 1291, 'plan': 996, 'deadly': 330, 'today': 1347, 'even': 428, 'fucking': 526, 'fuck': 525, 'drive': 385, 'road': 1121, 'kill': 704, 'explosion': 440, 'still': 1256, 'hear': 599, 'leader': 738, 'comment': 263, 'issue': 692, 'game': 540, 'effort': 403, 'win': 1482, 'aftershock': 19, 'price': 1034, 'im': 653, 'someone': 1218, 'also': 43, 'guess': 576, 'one': 939, 'actually': 14, 'free': 519, 'best': 130, 'ever': 430, 'minute': 846, 'daily': 317, 'could': 286, 'really': 1069, 'many': 802, 'already': 42, 'global': 557, 'financial': 483, 'meltdown': 826, 'moment': 854, 'guy': 580, 'behind': 127, 'scream': 1157, 'bloody': 154, 'murder': 870, 'full': 529, 'stream': 1265, 'youtube': 1522, 'sometimes': 1220, 'face': 446, 'wrong': 1507, 'thing': 1329, 'stand': 1248, 'dream': 383, 'possible': 1019, 'brown': 183, 'remember': 1086, 'avoid': 100, 'trap': 1371, 'think': 1330, 'job': 697, 'never': 889, 'kick': 702, 'say': 1152, 'cannot': 207, 'do': 374, 'anyone': 65, 'need': 888, 'play': 1001, 'expert': 438, 'france': 517, 'begin': 126, 'airplane': 28, 'debris': 333, 'find': 484, 'reunion': 1105, 'island': 688, 'french': 520, 'air': 23, 'debris find': 334, 'find reunion': 486, 'reunion island': 1106, 'pilot': 992, 'common': 264, 'airplane accident': 29, 'family': 454, 'member': 827, 'bin': 135, 'suspect': 1297, 'via': 1421, 'four': 516, 'men': 829, 'include': 658, 'state': 1252, 'government': 566, 'official': 927, 'wednesday': 1465, 'almost': 39, 'send': 1172, 'mode': 850, 'might': 835, 'wreck': 1503, 'plane': 998, 'plane crash': 999, 'wtf': 1509, 'ûªt': 1526, 'believe': 128, 'eye': 444, 'victim': 1423, 'ago': 20, 'little': 765, 'bit': 140, 'trauma': 1373, 'omg': 938, 'phone': 982, 'ship': 1186, 'look like': 776, 'cop': 282, 'house': 641, 'worry': 1497, 'airport': 30, 'early': 394, 'wake': 1439, 'call': 203, 'sister': 1205, 'ambulance': 45, 'hospital': 636, 'twelve': 1394, 'fear': 470, 'pakistani': 955, 'helicopter': 606, 'twelve fear': 1395, 'fear kill': 471, 'kill pakistani': 705, 'pakistani air': 956, 'air ambulance': 24, 'ambulance helicopter': 46, 'helicopter crash': 607, 'serious': 1176, 'reuters': 1108, 'lead': 736, 'service': 1177, 'welcome': 1469, 'emergency service': 411, 'incident': 657, 'ebay': 399, 'target': 1309, 'destroy': 356, 'blood': 153, 'crazy': 297, 'fight': 480, 'couple': 289, 'run': 1133, 'lucky': 788, 'ok': 933, 'pakistan': 954, 'em': 408, 'petition': 980, 'hour': 640, 'ûª': 1524, 'lot': 781, 'last year': 729, 'ûï': 1528, 'dog': 375, 'respond': 1100, 'siren': 1204, 'number': 913, 'body': 160, 'surprise': 1294, 'practice': 1025, 'trust': 1384, 'hate': 593, 'episode': 420, 'annihilate': 55, 'show': 1193, 'nigga': 901, 'hey': 612, 'career': 212, 'everything': 433, 'sorry': 1224, 'pull': 1044, 'drink': 384, 'driver': 386, 'safety': 1138, 'hit': 623, 'must': 875, 'country': 287, 'well': 1470, 'promise': 1039, 'israel': 689, 'horror': 633, 'iran': 684, 'since': 1200, 'history': 622, 'become': 124, 'whole': 1475, 'fact': 448, 'short': 1189, 'ball': 108, 'ready': 1067, 'weather': 1463, 'thought': 1333, 'feat': 472, 'review': 1109, 'case': 214, 'survivor': 1296, 'project': 1038, 'match': 815, 'syrian': 1302, 'army': 78, 'pile': 991, 'food': 506, 'bc': 119, 'fun': 532, 'bar': 113, 'hell': 608, 'total': 1356, 'annihilation': 56, 'destruction': 357, 'usa': 1415, 'maybe': 819, 'river': 1118, 'national': 879, 'park': 965, 'salt': 1141, 'wild': 1477, 'horse': 635, 'national forest': 880, 'stop annihilation': 1261, 'salt river': 1142, 'wild horse': 1479, 'world': 1496, 'self': 1171, 'attack': 92, 'human': 643, 'sign': 1197, 'share': 1184, 'save': 1148, 'without': 1488, 'soul': 1226, 'mention': 830, 'major': 793, 'law': 735, 'false': 453, 'fuel': 527, 'less': 748, 'away': 101, 'allow': 37, 'book': 170, 'join': 698, 'follow': 505, 'zone': 1523, 'soon': 1223, 'survive': 1295, 'apocalypse': 68, 'feel': 476, 'poor': 1017, 'boy': 172, 'child': 235, 'birthday': 139, 'watch': 1452, 'film': 481, 'august': 95, 'red': 1074, 'august pm': 96, 'stage': 1247, 'feel like': 477, 'kinda': 708, 'hot': 639, 'radio': 1054, 'disease': 371, 'start': 1251, 'lol': 770, 'question': 1051, 'version': 1420, 'high': 614, 'mountain': 861, 'imagine': 655, 'dad': 316, 'buy': 196, 'movie': 863, 'scar': 1153, 'storm': 1262, 'late': 730, 'totally': 1357, 'give': 554, 'bad': 106, 'name': 878, 'dark': 323, 'queen': 1050, 'enjoy': 415, 'like video': 758, 'armageddon': 77, 'peace': 976, 'bed': 125, 'full read': 530, 'read ebay': 1066, 'series': 1175, 'beat': 122, 'girl': 553, 'long': 772, 'hand': 586, 'bear': 121, 'sense': 1174, 'door': 377, 'data': 324, 'base': 114, 'tomorrow': 1351, 'point': 1010, 'department': 347, 'pay': 974, 'warn': 1448, 'escape': 422, 'rule': 1132, 'earth': 395, 'let': 749, 'ûªs': 1525, 'toddler': 1348, 'saw': 1150, 'hail': 582, 'year ago': 1513, 'el': 405, 'extremely': 443, 'till': 1344, 'united': 1410, 'class': 243, 'prepare': 1029, 'crisis': 305, 'part': 967, 'collapse': 254, 'track': 1360, 'light': 755, 'funny': 533, 'pick': 986, 'fan': 459, 'pick fan': 988, 'fan army': 460, 'one direction': 940, 'friend': 522, 'vote': 1437, 'round': 1128, 'hero': 611, 'tv': 1392, 'sport': 1241, 'blue': 157, 'date': 326, 'indian': 661, 'hope': 630, 'wwii': 1510, 'japanese': 696, 'military': 839, 'japan': 693, 'leather': 741, 'war': 1445, 'learn': 739, 'violent': 1430, 'control': 279, 'terrorist': 1322, 'charge': 230, 'arson': 83, 'truth': 1385, 'struggle': 1274, 'add': 15, 'crime': 304, 'link': 761, 'catch': 219, 'northern': 904, 'notice': 908, 'sound': 1227, 'gay': 542, 'northern california': 905, 'death': 332, 'palestinian': 957, 'amid': 49, 'fail': 449, 'shoulder': 1191, 'pray': 1028, 'white': 1474, 'east': 397, 'bay': 116, 'arrest': 81, 'system': 1303, 'anything': 66, 'nice': 898, 'sit': 1206, 'capture': 209, 'remove': 1087, 'american': 48, 'beach': 120, 'ca': 197, 'town': 1359, 'ice': 649, 'blame': 144, 'blaze': 147, 'business': 195, 'cant': 208, 'green': 571, 'court': 291, 'lmao': 768, 'real': 1068, 'terrorism': 1321, 'big': 133, 'burning': 189, 'true': 1383, 'story': 1263, 'miss': 848, 'stay': 1253, 'theater': 1328, 'gun': 578, 'terror': 1319, 'post': 1021, 'terror attack': 1320, 'police post': 1013, 'million': 840, 'nuclear': 909, 'warning': 1449, 'claim': 242, 'suicide': 1283, 'bomb': 163, 'saudi': 1146, 'mosque': 858, 'suicide bomb': 1284, 'twitter': 1397, 'blast': 145, 'militant': 838, 'udhampur': 1403, 'injure': 665, 'obama': 915, 'weapon': 1461, 'texas': 1324, 'steal': 1254, 'internet': 675, 'new post': 893, 'israeli': 690, 'force': 510, 'okay': 934, 'lie': 752, 'shift': 1185, 'training': 1368, 'yet': 1516, 'response': 1102, 'yeah': 1511, 'damn': 319, 'india': 660, 'release': 1083, 'woman': 1490, 'team': 1310, 'act': 12, 'atomic': 89, 'atomic bomb': 90, 'group': 574, 'gop': 564, 'feeling': 478, 'medium': 823, 'civilian': 241, 'complete': 267, 'christian': 238, 'muslims': 873, 'temple': 1314, 'mount': 859, 'pamela': 958, 'geller': 543, 'christian attack': 239, 'attack muslims': 93, 'muslims temple': 874, 'temple mount': 1315, 'mount wave': 860, 'wave israeli': 1459, 'israeli flag': 691, 'flag via': 496, 'via pamela': 1422, 'pamela geller': 959, 'unit': 1409, 'youth': 1520, 'literally': 764, 'person': 979, 'view': 1427, 'answer': 61, 'tweet': 1393, 'program': 1037, 'fly': 504, 'mine': 842, 'fukushima': 528, 'avalanche': 98, 'louis': 784, 'king': 709, 'bet': 131, 'box': 171, 'piece': 990, 'write': 1506, 'design': 351, 'album': 35, 'cover': 292, 'music': 872, 'beautiful': 123, 'calgary': 200, 'flame': 497, 'create': 298, 'worth': 1498, 'fully': 531, 'gas': 541, 'lift': 754, 'favorite': 469, 'deal': 331, 'song': 1222, 'yo': 1517, 'star': 1250, 'power': 1023, 'battle': 115, 'general': 546, 'playlist': 1003, 'add video': 16, 'video playlist': 1426, 'sure': 1293, 'space': 1231, 'occur': 920, 'fleet': 501, 'space battle': 1232, 'rip': 1114, 'young': 1519, 'mass': 811, 'australia': 97, 'son': 1221, 'fedex': 473, 'longer': 773, 'transport': 1369, 'bioterror': 136, 'germ': 547, 'anthrax': 62, 'lab': 716, 'mishap': 847, 'fedex longer': 474, 'longer transport': 774, 'transport bioterror': 1370, 'bioterror germ': 137, 'germ wake': 548, 'wake anthrax': 1440, 'anthrax lab': 63, 'lab mishap': 717, 'potential': 1022, 'research': 1099, 'cut': 314, 'hold': 624, 'problem': 1036, 'concern': 270, 'action': 13, 'bioterrorism': 138, 'region': 1082, 'carry': 213, 'hostage': 637, 'keep': 701, 'public': 1043, 'health': 598, 'outbreak': 946, 'security': 1164, 'climate': 246, 'climate change': 247, 'hollywood': 626, 'yes': 1515, 'disaster': 367, 'pretty': 1032, 'list': 762, 'special': 1236, 'future': 536, 'threat': 1335, 'ahead': 22, 'test': 1323, 'event': 429, 'pool': 1016, 'wanna': 1443, 'hair': 584, 'weekend': 1467, 'loss': 780, 'nearly': 887, 'california wildfire': 202, 'firefighter': 490, 'put': 1046, 'pic': 984, 'computer': 268, 'mad': 789, 'dont': 376, 'seek': 1167, 'experience': 437, 'agree': 21, 'party': 968, 'silver': 1199, 'gem': 544, 'nothing': 907, 'online': 941, 'old': 936, 'ash': 87, 'lady': 718, 'picture': 989, 'oh': 930, 'word': 1493, 'cold': 253, 'enough': 416, 'degree': 340, 'sun': 1287, 'beyond': 132, 'dude': 390, 'bag': 107, 'bitch': 141, 'cake': 199, 'throw': 1338, 'bleed': 148, 'text': 1325, 'tear': 1311, 'heard': 600, 'foot': 508, 'glass': 556, 'pain': 953, 'character': 229, 'brain': 174, 'wed': 1464, 'blew': 149, 'line': 760, 'blow': 155, 'entire': 418, 'wind': 1483, 'blight': 150, 'search': 1159, 'deep': 339, 'info': 663, 'level': 750, 'matter': 816, 'result': 1103, 'community': 265, 'policy': 1014, 'land': 722, 'open': 942, 'tragedy': 1362, 'article': 85, 'estimate': 423, 'blizzard': 151, 'rock': 1122, 'hi': 613, 'roll': 1124, 'though': 1332, 'window': 1485, 'idea': 650, 'tho': 1331, 'walk': 1441, 'cook': 280, 'wall': 1442, 'morning': 857, 'hurt': 647, 'wound': 1500, 'standard': 1249, 'large': 726, 'super': 1290, 'drown': 389, 'gotta': 565, 'sad': 1137, 'lightning': 756, 'enter': 417, 'listen': 763, 'russia': 1134, 'effect': 401, 'low': 786, 'money': 855, 'oil': 931, 'sink': 1202, 'meet': 825, 'friday': 521, 'suppose': 1292, 'arm': 76, 'killer': 707, 'rather': 1059, 'blown': 156, 'wow': 1502, 'player': 1002, 'probably': 1035, 'strike': 1269, 'crew': 303, 'ur': 1414, 'soldier': 1217, 'panic': 962, 'mind': 841, 'glad': 555, 'half': 585, 'ûªve': 1527, 'as': 86, 'tote': 1358, 'handbag': 587, 'faux': 467, 'purse': 1045, 'new lady': 892, 'lady shoulder': 719, 'shoulder tote': 1192, 'cross body': 308, 'body bag': 161, 'faux leather': 468, 'along': 41, 'coach': 250, 'tablet': 1304, 'rise': 1115, 'drake': 382, 'meek': 824, 'straight': 1264, 'together': 1350, 'spot': 1242, 'mark': 807, 'record': 1071, 'dangerous': 322, 'appear': 71, 'pack': 952, 'football': 509, 'room': 1125, 'metal': 832, 'small': 1212, 'russian': 1135, 'middle': 834, 'giant': 551, 'size': 1209, 'colour': 259, 'investigate': 679, 'washington': 1451, 'hiroshima': 620, 'drop': 387, 'gunman': 579, 'board': 158, 'baby': 103, 'impact': 656, 'thursday': 1342, 'anniversary': 57, 'coast': 252, 'centre': 226, 'bell': 129, 'libya': 751, 'islamic': 686, 'turkey': 1388, 'islamic state': 687, 'ground': 573, 'main': 792, 'forget': 514, 'village': 1428, 'co': 249, 'bomb japan': 164, 'nagasaki': 877, 'nuclear weapon': 912, 'accord': 9, 'feed': 475, 'pkk': 993, 'bomb turkey': 165, 'japan mark': 694, 'hurricane': 646, 'guide': 577, 'bombing': 169, 'mark anniversary': 808, 'atomic bombing': 91, 'anniversary hiroshima': 59, 'bridge': 177, 'trent': 1379, 'trent bridge': 1380, 'bridge collapse': 178, 'injured': 666, 'central': 225, 'five': 493, 'natural': 881, 'crane': 293, 'nearby': 884, 'two giant': 1399, 'giant crane': 552, 'crane hold': 294, 'hold bridge': 625, 'collapse nearby': 255, 'nearby home': 886, 'aug': 94, 'mp': 865, 'stuff': 1277, 'currently': 313, 'likely': 759, 'rescuer': 1097, 'rescuer search': 1098, 'burn building': 188, 'etc': 424, 'burning building': 190, 'rocky': 1123, 'threaten': 1336, 'fire burn': 488, 'rioting': 1113, 'western': 1472, 'riot': 1112, 'ruin': 1131, 'outrage': 948, 'rescue': 1095, 'explode': 439, 'equipment': 421, 'hazard': 595, 'downtown': 380, 'building fire': 186, 'ross': 1127, 'fire truck': 489, 'different': 365, 'evacuate': 425, 'grill': 572, 'insurance': 670, 'alarm': 34, 'remain': 1085, 'camp': 205, 'displace': 372, 'confirm': 273, 'brother': 182, 'content': 276, 'shower': 1194, 'mph': 867, 'social': 1216, 'spark': 1235, 'return': 1104, 'shape': 1183, 'desire': 352, 'boat': 159, 'memory': 828, 'apply': 73, 'parent': 964, 'press': 1031, 'understand': 1407, 'ticket': 1343, 'sound like': 1228, 'ppl': 1024, 'sex': 1181, 'cancer': 206, 'murderer': 871, 'express': 441, 'bush': 193, 'bush fire': 194, 'volcano': 1435, 'republican': 1094, 'freak': 518, 'drought': 388, 'seem': 1169, 'traumatise': 1374, 'swim': 1301, 'water': 1454, 'catastrophic': 218, 'affect': 17, 'casualty': 215, 'reduce': 1077, 'ban': 109, 'account': 10, 'laugh': 733, 'operation': 943, 'fix': 494, 'reach': 1063, 'derailment': 349, 'train derailment': 1366, 'course': 290, 'rate': 1058, 'ocean': 921, 'population': 1018, 'chance': 227, 'industry': 662, 'strong': 1270, 'insurer': 671, 'ten': 1316, 'catastrophe': 217, 'suffer': 1282, 'continue': 278, 'failure': 450, 'prevent': 1033, 'company': 266, 'repair': 1088, 'emotional': 412, 'anymore': 64, 'felt': 479, 'hiroshima nagasaki': 621, 'replace': 1091, 'british': 180, 'clear': 244, 'hazardous': 596, 'chemical': 233, 'instead': 669, 'harm': 591, 'spill': 1239, 'plant': 1000, 'emergency plan': 410, 'responder': 1101, 'cliff': 245, 'fall cliff': 452, 'china': 237, 'landslide': 724, 'knock': 711, 'interview': 676, 'absolutely': 7, 'image': 654, 'president': 1030, 'angry': 53, 'animal': 54, 'cost': 283, 'collide': 256, 'step': 1255, 'thunderstorm': 1340, 'either': 404, 'gold': 561, 'two train': 1400, 'exchange': 434, 'dan': 320, 'san': 1143, 'collision': 257, 'local': 769, 'approach': 74, 'fatal': 463, 'download': 379, 'sea': 1158, 'crush': 309, 'cable': 198, 'stress': 1267, 'curfew': 312, 'cat': 216, 'wonder': 1492, 'cyclone': 315, 'severe': 1179, 'typhoon': 1401, 'soudelor': 1225, 'devastate': 361, 'delivers': 342, 'union': 1408, 'tree': 1376, 'cause damage': 221, 'isi': 685, 'danger': 321, 'come soon': 262, 'govt': 569, 'rail': 1055, 'aircraft': 25, 'hundred': 644, 'migrant': 836, 'taiwan': 1305, 'typhoon soudelor': 1402, 'sue': 1280, 'legionnaire': 743, 'legionnaire disease': 745, 'loose': 777, 'usually': 1417, 'minister': 845, 'increase': 659, 'malaysia': 795, 'airline': 27, 'flight': 502, 'malaysia airline': 796, 'aircraft debris': 26, 'find la': 485, 'la reunion': 715, 'reunion miss': 1107, 'miss malaysia': 849, 'wash': 1450, 'york': 1518, 'new york': 894, 'official say': 928, 'abc': 3, 'interest': 672, 'deluge': 344, 'stock': 1258, 'uk': 1404, 'rise top': 1116, 'top pay': 1354, 'quiz': 1053, 'low take': 787, 'take quiz': 1307, 'patience': 972, 'demolish': 345, 'enugu': 419, 'structure': 1273, 'international': 674, 'former': 515, 'demolition': 346, 'med': 822, 'kit': 710, 'derail': 348, 'train derail': 1365, 'madhya': 790, 'pradesh': 1026, 'madhya pradesh': 791, 'passenger': 969, 'pradesh train': 1027, 'derailment village': 350, 'village youth': 1429, 'youth save': 1521, 'save many': 1149, 'many life': 804, 'desolate': 353, 'desolation': 354, 'smaug': 1213, 'desolation smaug': 355, 'trouble': 1381, 'hat': 592, 'detonate': 358, 'apollo': 69, 'ft': 524, 'apollo brown': 70, 'detonate bomb': 359, 'message': 831, 'ignition': 651, 'detonation': 360, 'ignition knock': 652, 'knock detonation': 712, 'declare': 337, 'saipan': 1139, 'declaration': 335, 'obama declare': 916, 'declare disaster': 338, 'disaster saipan': 369, 'saipan obama': 1140, 'obama sign': 917, 'sign disaster': 1198, 'disaster declaration': 368, 'declaration northern': 336, 'marians': 806, 'northern marians': 906, 'devastation': 362, 'wrought': 1508, 'year atomic': 1514, 'japan still': 695, 'still struggle': 1257, 'struggle war': 1275, 'war past': 1446, 'past anniversary': 971, 'anniversary devastation': 58, 'devastation wrought': 363, 'natural disaster': 882, 'displaced': 373, 'refugee': 1078, 'internally': 673, 'philippine': 981, 'extreme': 442, 'nigerian': 899, 'loud': 782, 'information': 664, 'hundred migrant': 645, 'dust': 392, 'dust storm': 393, 'thunder': 1339, 'utc': 1418, 'time utc': 1346, 'earthquake': 396, 'seismic': 1170, 'oklahoma': 935, 'hawaii': 594, 'volcano hawaii': 1436, 'tsunami': 1387, 'electrocute': 406, 'map': 805, 'cree': 299, 'reddit': 1075, 'severe thunderstorm': 1180, 'nw': 914, 'sick': 1195, 'brooklyn': 181, 'engulfed': 414, 'abandon': 2, 'roosevelt': 1126, 'eyewitness': 445, 'crematorium': 301, 'provoke': 1041, 'famine': 457, 'russian food': 1136, 'food crematorium': 507, 'crematorium provoke': 302, 'provoke outrage': 1042, 'outrage amid': 949, 'amid crisis': 50, 'crisis famine': 306, 'famine memory': 458, 'manslaughter': 799, 'sh': 1182, 'boy charge': 173, 'charge manslaughter': 231, 'manslaughter toddler': 801, 'toddler report': 1349, 'report boy': 1093, 'manslaughter fatal': 800, 'fatal sh': 465, 'investigator': 680, 'fatality': 466, 'combo': 260, 'first responder': 492, 'flatten': 500, 'offroad': 929, 'lamp': 721, 'cree lead': 300, 'lead work': 737, 'work light': 1495, 'trench': 1377, 'cdt': 223, 'pm cdt': 1008, 'apc': 67, 'bayelsa': 117, 'forest service': 513, 'thunderstorm warn': 1341, 'hailstorm': 583, 'hellfire': 609, 'tension': 1317, 'jonathan': 699, 'hijack': 615, 'pdp': 975, 'tension bayelsa': 1318, 'bayelsa patience': 118, 'patience jonathan': 973, 'jonathan plan': 700, 'plan hijack': 997, 'hijack apc': 616, 'specially': 1237, 'governor': 567, 'parole': 966, 'hijacker': 618, 'governor allow': 568, 'allow parole': 38, 'bus hijacker': 192, 'funtenna': 534, 'funtenna hijack': 535, 'hijack computer': 617, 'computer send': 269, 'send data': 1173, 'data sound': 325, 'sound wave': 1229, 'wave black': 1458, 'black hat': 143, 'wreckage': 1504, 'modify': 851, 'stadium': 1245, 'specially modify': 1238, 'modify land': 852, 'land stadium': 723, 'stadium rescue': 1246, 'rescue hostage': 1096, 'hostage iran': 638, 'massacre': 814, 'bang': 111, 'inundate': 677, 'inundated': 678, 'lava': 734, 'unconfirmed': 1405, 'news unconfirmed': 896, 'unconfirmed heard': 1406, 'heard loud': 601, 'loud bang': 783, 'bang nearby': 112, 'nearby appear': 885, 'appear blast': 72, 'blast wind': 146, 'wind as': 1484, 'mass murder': 812, 'mass murderer': 813, 'mayhem': 820, 'mudslide': 869, 'nuclear disaster': 910, 'reactor': 1064, 'nuclear reactor': 911, 'ancient': 52, 'obliterate': 918, 'obliteration': 919, 'oil spill': 932, 'refugio': 1080, 'costlier': 284, 'refugio oil': 1081, 'spill may': 1240, 'may costlier': 818, 'costlier big': 285, 'big project': 134, 'legionnaires': 746, 'disea': 370, 'family sue': 456, 'sue legionnaires': 1281, 'legionnaires family': 747, 'family affect': 455, 'affect fatal': 18, 'fatal outbreak': 464, 'outbreak legionnaire': 947, 'legionnaire disea': 744, 'pandemonium': 960, 'pandemonium aba': 961, 'aba woman': 1, 'woman delivers': 1491, 'delivers baby': 343, 'baby without': 104, 'without face': 1489, 'face photo': 447, 'panic attack': 963, 'alabama': 32, 'ebola': 400, 'police officer': 1012, 'quarantine': 1047, 'reddit quarantine': 1076, 'offensive': 922, 'quarantine offensive': 1048, 'offensive content': 923, 'content policy': 277, 'subreddits': 1278, 'new content': 891, 'policy go': 1015, 'go effect': 559, 'effect many': 402, 'many horrible': 803, 'horrible subreddits': 632, 'subreddits ban': 1279, 'ban quarantine': 110, 'alabama home': 33, 'quarantine possible': 1049, 'possible ebola': 1020, 'abc news': 4, 'rainstorm': 1057, 'raze': 1060, 'late home': 731, 'home raze': 629, 'raze northern': 1061, 'wildfire abc': 1481, 'turkish': 1390, 'cameroon': 204, 'repatriate': 1089, 'nigerian refugee': 900, 'refugee repatriate': 1079, 'repatriate cameroon': 1090, 'structural': 1271, 'video pick': 1425, 'pick body': 987, 'body water': 162, 'water rescuer': 1456, 'search hundred': 1160, 'rubble': 1130, 'stock market': 1259, 'market crash': 810, 'crash gem': 296, 'gem rubble': 545, 'sandstorm': 1144, 'swallow': 1299, 'watch airport': 1453, 'airport get': 31, 'get swallow': 550, 'swallow sandstorm': 1300, 'sandstorm minute': 1145, 'rly': 1119, 'seek comment': 1168, 'sinkhole': 1203, 'snowstorm': 1215, 'stretcher': 1268, 'virgin': 1432, 'galactic': 538, 'spaceship': 1233, 'investigator say': 682, 'virgin galactic': 1433, 'galactic spaceship': 539, 'spaceship crash': 1234, 'cause structural': 222, 'structural failure': 1272, 'bomber': 166, 'old pkk': 937, 'pkk suicide': 994, 'suicide bomber': 1285, 'bomber detonate': 167, 'pic old': 985, 'turkey army': 1389, 'army trench': 79, 'trench release': 1378, 'bomber kill': 168, 'kill saudi': 706, 'saudi security': 1147, 'security site': 1165, 'site mosque': 1208, 'sunk': 1289, 'miner': 843, 'recount': 1072, 'rly tragedy': 1120, 'tragedy mp': 1363, 'mp live': 866, 'live recount': 767, 'recount horror': 1073, 'ûïwhen': 1529, 'plunging': 1005, 'horror ûïwhen': 634, 'ûïwhen saw': 1530, 'saw coach': 1151, 'coach train': 251, 'train plunging': 1367, 'plunging water': 1006, 'water call': 1455, 'chile': 236, 'hollywood movie': 627, 'movie trap': 864, 'trap miner': 1372, 'miner release': 844, 'release chile': 1084, 'richmond': 1110, 'twister': 1396, 'upheaval': 1412, 'violent storm': 1431, 'war zone': 1447, 'whirlwind': 1473, 'wild fire': 1478, 'windstorm': 1486, 'officer wound': 926, 'wound suspect': 1501, 'suspect dead': 1298, 'dead exchange': 329, 'exchange shot': 435, 'conclusively': 271, 'wreckage conclusively': 1505, 'conclusively confirm': 272, 'confirm malaysia': 274, 'malaysia pm': 797, 'pm investigator': 1009, 'investigator family': 681}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_vec = Tfidf_vect.fit_transform(train['text_final'])\n",
    "text_vec_test = Tfidf_vect.transform(test['text_final'])\n",
    "X_train_text = pd.DataFrame(text_vec.toarray(), columns=Tfidf_vect.get_feature_names())\n",
    "X_test_text = pd.DataFrame(text_vec_test.toarray(), columns=Tfidf_vect.get_feature_names())\n",
    "\n",
    "train = train.join(X_train_text, rsuffix='_text')\n",
    "test = test.join(X_test_text, rsuffix='_text')\n",
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 107)\n"
     ]
    }
   ],
   "source": [
    "vec_hash = CountVectorizer(min_df = 5)\n",
    "hash_vec = vec_hash.fit_transform(train['hashtags'])\n",
    "hash_vec_test = vec_hash.transform(test['hashtags'])\n",
    "X_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\n",
    "X_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())\n",
    "print (X_train_hash.shape)\n",
    "\n",
    "train = train.join(X_train_hash, rsuffix='_hashtag')\n",
    "test = test.join(X_test_hash, rsuffix='_hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags into a new column\n",
    "temp=[]\n",
    "for i in range(len(train.text_lower)):\n",
    "    temp.append(re.findall(r'#(\\w+)', train.text_lower[i]))\n",
    "train['text_hash']=temp\n",
    "\n",
    "temp=[]\n",
    "for i in range(len(test.text_lower)):\n",
    "    temp.append(re.findall(r'#(\\w+)', test.text_lower[i]))\n",
    "test['text_hash']=temp\n",
    "\"\"\"\n",
    "\n",
    "# Make string\n",
    "for i in range(len(train.text_hash)):\n",
    "    train.text_hash[i]=str(train.text_hash[i])\n",
    "    \n",
    "# Replace empty list is text_hash column\n",
    "for i in range(len(test.text_hash)):\n",
    "    test.text_hash[i]=str(test.text_hash[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-append processed hashtags\n",
    "for i in range(len(train.text_final)):\n",
    "    train.text_hash[i]=[t for t in train.text_hash[i] if t.isalpha() and t not in stopwords.words('english')]\n",
    "    train.text_hash[i]=[lemmatizer.lemmatize(word, nltk_tag_to_wordnet_tag(tag)) for word,tag in pos_tag(train.text_hash[i])]\n",
    "    train.text_final[i].extend(train.text_hash[i])\n",
    "    train.text_final[i]=str(train.text_final[i])\n",
    "    \n",
    "for i in range(len(test.text_final)):\n",
    "    test.text_hash[i]=[t for t in test.text_hash[i] if t.isalpha() and t not in stopwords.words('english')]\n",
    "    test.text_hash[i]=[lemmatizer.lemmatize(word, nltk_tag_to_wordnet_tag(tag)) for word,tag in pos_tag(test.text_hash[i])]\n",
    "    test.text_final[i].extend(test.text_hash[i])\n",
    "    test.text_final[i]=str(test.text_final[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out other classification models\n",
    "\n",
    "* Using hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['id', 'keyword','location','text', 'target','text_clean', 'hashtags', \n",
    "                    'mentions','links', 'emojis', 'text_lower', 'text_token', 'text_final']\n",
    "X_train = train.drop(columns=features_to_drop+['target','target_text'])\n",
    "X_test = test.drop(columns=features_to_drop)\n",
    "y_train = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'logistic_regression' : LogisticRegression, \n",
    "   'rf' : RandomForestClassifier, \n",
    "   'naive_bayes' : BernoulliNB, 'svc' : SVC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_space(model):  \n",
    "    model = model.lower()\n",
    "    space = {}\n",
    "    if model == 'naive_bayes':\n",
    "        space = {'alpha': hp.choice('alpha', [0,1])\n",
    "                }\n",
    "    elif model == 'svc':\n",
    "        space = {'C': hp.lognormal('C', 0, 1.0),\n",
    "                 'kernel': hp.choice('kernel', ['linear', 'sigmoid', 'poly', 'rbf']),\n",
    "                 'gamma': hp.uniform('gamma', 0, 20)\n",
    "                }\n",
    "    elif model == 'logistic_regression':\n",
    "        space = {'warm_start' : hp.choice('warm_start', [True, False]), \n",
    "                 'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
    "                 'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
    "                 'C' : hp.uniform('C', 0.05, 3),\n",
    "                 'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear']),\n",
    "                 'max_iter' : hp.choice('max_iter', range(100,1000)),\n",
    "                 'class_weight' : 'balanced',\n",
    "                 'n_jobs' : -1\n",
    "                }\n",
    "    elif model == 'rf':\n",
    "        space = {'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "                 'n_estimators': hp.choice('n_estimators', range(50,300)),\n",
    "                 #'n_estimators': 150,\n",
    "                 #'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "                 'criterion' : 'gini',\n",
    "                 'min_samples_split': hp.choice(\"min_samples_split\", range(2,40)),\n",
    "                 'n_jobs' : -1\n",
    "                }\n",
    "    space['model'] = model\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_status(clf,X,y):\n",
    "    acc = cross_val_score(clf, X, y, cv=3, scoring='f1').mean()\n",
    "    #y_pred = clf.fit(X,y).predict(X_test)\n",
    "    #print(confusion_matrix(y_hold, y_pred))\n",
    "    #print(classification_report(y_hold, y_pred))\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fnc(params) :\n",
    "    model = params.get('model').lower()\n",
    "    del params['model']\n",
    "    clf = models[model](**params)\n",
    "    return(get_acc_status(clf,X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= 'rf'\n",
    "best_params = fmin(obj_fnc, \n",
    "                   search_space(model), \n",
    "                   algo=tpe.suggest, \n",
    "                   max_evals=100)\n",
    " \n",
    "print(best_params)\n",
    "# with bigrams\n",
    "#{'max_depth': 18, 'min_samples_split': 6, 'n_estimators': 182}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(criterion='gini', max_depth= 18, min_samples_split = 0.5070744524836673, n_estimators= 157)\n",
    "y_pred = rf.fit(X_train, y_train).predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= 'logistic_regression'\n",
    "best_params = fmin(obj_fnc, \n",
    "                   search_space(model), \n",
    "                   algo=tpe.suggest, \n",
    "                   max_evals=100)\n",
    " \n",
    "print(best_params)\n",
    "#first\n",
    "#{'C': 0.8625782737995314, 'fit_intercept': True, 'max_iter': 318, 'solver': 'lbfgs', \n",
    "# 'tol': 3.3702355408712684e-05, 'warm_start': False}\n",
    "# with bigrams\n",
    "#{'C': 0.4191297475916065, 'fit_intercept': True, 'max_iter': 39, 'solver': 'liblinear', \n",
    "# 'tol': 6.003262415227273e-05, 'warm_start': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3987  355]\n",
      " [ 597 2674]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89      4342\n",
      "           1       0.88      0.82      0.85      3271\n",
      "\n",
      "    accuracy                           0.87      7613\n",
      "   macro avg       0.88      0.87      0.87      7613\n",
      "weighted avg       0.88      0.87      0.87      7613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regresion=LogisticRegression(C=0.4191297475916065, fit_intercept=True, max_iter = 39, solver='liblinear', \n",
    "                                      tol = 6.003262415227273e-05, warm_start=False)\n",
    "y_pred = logistic_regresion.fit(X_train, y_train).predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= 'naive_bayes'\n",
    "best_params = fmin(obj_fnc, \n",
    "                   search_space(model), \n",
    "                   algo=tpe.suggest, \n",
    "                   max_evals=100)\n",
    " \n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes=BernoulliNB()\n",
    "y_pred = naive_bayes.fit(X_train, y_train).predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= 'svc'\n",
    "best_params = fmin(obj_fnc, \n",
    "                   search_space(model), \n",
    "                   algo=tpe.suggest, \n",
    "                   max_evals=100)\n",
    " \n",
    "print(best_params)\n",
    "#first\n",
    "#{'C': 1.5426972107125763, 'gamma': 0.9018573366743587, 'kernel': 'rbf'}\n",
    "# with bigrams\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC(C = 1.5426972107125763, gamma = 0.9018573366743587, kernel = 'rbf')\n",
    "y_pred = svc.fit(X_train, y_train).predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in X_train.columns:\n",
    "    if val not in X_test.columns:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regresion=LogisticRegression(C=0.4191297475916065, fit_intercept=True, max_iter = 39, solver='liblinear', \n",
    "                                      tol = 6.003262415227273e-05, warm_start=False)\n",
    "y_pred = logistic_regresion.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['target'] = y_pred\n",
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak = pd.read_csv(\"../input/realornot/socialmedia-disaster-tweets-DFE.csv\", encoding='latin_1')\n",
    "leak['target'] = (leak['choose_one']=='Relevant').astype(int)\n",
    "leak['id'] = leak.index\n",
    "leak = leak[['id', 'target','text']]\n",
    "merged_df = pd.merge(test, leak, on='id')\n",
    "sub1 = merged_df[['id', 'target']]\n",
    "sub1.to_csv('submit_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "3258    0.0\n",
       "3259    0.0\n",
       "3260    0.0\n",
       "3261    0.0\n",
       "3262    0.0\n",
       "Name: target, Length: 3263, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(merged_df.target_x!=merged_df.target_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.444px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
